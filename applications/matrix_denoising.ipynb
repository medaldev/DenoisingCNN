{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-06T00:55:07.531798204Z",
     "start_time": "2023-11-06T00:55:04.997642201Z"
    }
   },
   "outputs": [],
   "source": [
    "from applications import image_denoising\n",
    "from models import ConvAutoencoderLuma\n",
    "from dataloaders import SimpleLoader2d\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = 'cpu' #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = ConvAutoencoderLuma().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T00:55:21.891473991Z",
     "start_time": "2023-11-06T00:55:21.884535220Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "name_model = \"model_1\"\n",
    "name_dataset = \"gcg\"\n",
    "path_base = \"/home/amedvedev/fprojects/cpp/gcg_2d_minimal/data/\"\n",
    "\n",
    "path_save_model = os.path.join(\"../assets/pt/\", f\"{name_dataset}_{name_model}.pt\")\n",
    "\n",
    "try:\n",
    "    model = torch.load(path_save_model).to(device)\n",
    "except Exception as e:\n",
    "    print(\"Error when loading pretrained model. Use custom.\")\n",
    "    model = ConvAutoencoderLuma().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T00:55:19.743374136Z",
     "start_time": "2023-11-06T00:55:19.736522224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "path_save_train_plots = os.path.join(\"../assets/runs/train_plots\", name_dataset, name_model)\n",
    "\n",
    "path_train = os.path.join(path_base, name_dataset, \"train\")\n",
    "path_test = os.path.join(path_base, name_dataset, \"val\")\n",
    "\n",
    "path_train_rainy = path_train + \"/noised\"\n",
    "path_test_rainy = path_test + \"/noised\"\n",
    "\n",
    "path_train_normal = path_train + \"/clear\"\n",
    "path_test_normal = path_test + \"/clear\"\n",
    "\n",
    "\n",
    "width, height = 100, 100\n",
    "\n",
    "train_noisy_loader = SimpleLoader2d(path_train_rainy, device, BATCH_SIZE, width, height)\n",
    "train_normal_loader = SimpleLoader2d(path_train_normal, device, BATCH_SIZE, width, height)\n",
    "\n",
    "val_noisy_loader =SimpleLoader2d(path_test_rainy, device, BATCH_SIZE, width, height)\n",
    "val_normal_loader =SimpleLoader2d(path_test_normal, device, BATCH_SIZE, width, height)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T00:55:52.406358649Z",
     "start_time": "2023-11-06T00:55:52.375175736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def plot_step_results():\n",
    "    image_denoising.save_results(model, device, path_save_train_plots, train_noisy_loader, train_normal_loader, \"train\",\n",
    "                                 limit=1, nrow=8, op_count=1)\n",
    "    image_denoising.save_results(model, device, path_save_train_plots, val_noisy_loader, val_normal_loader, \"test\",\n",
    "                                 limit=1, nrow=8, op_count=1)\n",
    "    print(f\"Step results plotted to {path_save_train_plots}.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T00:55:55.002947916Z",
     "start_time": "2023-11-06T00:55:54.998658280Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step results plotted to ../assets/runs/train_plots/gcg/model_1.\n",
      "Training progress: |██████████████████████████████████████████████████| 100.0% Complete | \r\n",
      "\n",
      "Validating progress: |██████████████████████████████████████████████████| 100.0% Complete | \r\n",
      "\n",
      "Epoch: 1 \tTraining Loss: 0.509784 \tValidating Loss: 0.048046 \tTime: 1.53 m\n",
      "Step results plotted to ../assets/runs/train_plots/gcg/model_1.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Training progress: |██████████████████████████████████████████████████| 100.0% Complete | \r\n",
      "\n",
      "Validating progress: |██████████████████████████████████████████████████| 100.0% Complete | \r\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.045001 \tValidating Loss: 0.042642 \tTime: 1.51 m\n",
      "Model saved successfully at ../assets/pt/gcg_model_1.pt.\n",
      "Step results plotted to ../assets/runs/train_plots/gcg/model_1.\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "image_denoising.train(model, train_noisy_loader, train_normal_loader, val_noisy_loader, val_normal_loader,\n",
    "                      EPOCHS, device, path_save=path_save_model, callbacks=[plot_step_results])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T00:59:44.140614072Z",
     "start_time": "2023-11-06T00:56:35.304300071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_step_results()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_denoising.save_full_model(model, path_save_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [76, 1, 3, 3], expected input[1, 3, 100, 100] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m path_save_model_onnx \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../assets/onnx/\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname_dataset\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname_model\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.onnx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mimage_denoising\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_onnx_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath_save_model_onnx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/applications/image_denoising.py:168\u001B[0m, in \u001B[0;36msave_onnx_model\u001B[0;34m(model, path_model, device, image_size)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    166\u001B[0m     inp \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, image_size[\u001B[38;5;241m0\u001B[39m], image_size[\u001B[38;5;241m1\u001B[39m]), device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m--> 168\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m                      \u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mpath_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m                      \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m14\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;129m@_beartype\u001B[39m\u001B[38;5;241m.\u001B[39mbeartype\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexport\u001B[39m(\n\u001B[1;32m    191\u001B[0m     model: Union[torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule, torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mScriptModule, torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mScriptFunction],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    208\u001B[0m     autograd_inlining: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    209\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001B[39;00m\n\u001B[1;32m    211\u001B[0m \n\u001B[1;32m    212\u001B[0m \u001B[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001B[39;00m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m     \u001B[43m_export\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograd_inlining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautograd_inlining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/onnx/utils.py:1596\u001B[0m, in \u001B[0;36m_export\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001B[0m\n\u001B[1;32m   1593\u001B[0m     dynamic_axes \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1594\u001B[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001B[0;32m-> 1596\u001B[0m graph, params_dict, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_model_to_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1597\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1598\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1599\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1600\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1601\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1602\u001B[0m \u001B[43m    \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1603\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_do_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1604\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1605\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1606\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1607\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1609\u001B[0m \u001B[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001B[39;00m\n\u001B[1;32m   1610\u001B[0m defer_weight_export \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1611\u001B[0m     export_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _exporter_states\u001B[38;5;241m.\u001B[39mExportTypes\u001B[38;5;241m.\u001B[39mPROTOBUF_FILE\n\u001B[1;32m   1612\u001B[0m )\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001B[0m, in \u001B[0;36m_model_to_graph\u001B[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001B[0m\n\u001B[1;32m   1132\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args,)\n\u001B[1;32m   1134\u001B[0m model \u001B[38;5;241m=\u001B[39m _pre_trace_quant_model(model, args)\n\u001B[0;32m-> 1135\u001B[0m graph, params, torch_out, module \u001B[38;5;241m=\u001B[39m \u001B[43m_create_jit_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1136\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m _get_named_param_dict(graph, params)\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001B[0m, in \u001B[0;36m_create_jit_graph\u001B[0;34m(model, args)\u001B[0m\n\u001B[1;32m   1006\u001B[0m     graph \u001B[38;5;241m=\u001B[39m _C\u001B[38;5;241m.\u001B[39m_propagate_and_assign_input_shapes(\n\u001B[1;32m   1007\u001B[0m         graph, flattened_args, param_count_list, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1008\u001B[0m     )\n\u001B[1;32m   1009\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m graph, params, torch_out, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1011\u001B[0m graph, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_trace_and_get_graph_from_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1012\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[1;32m   1013\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39m_unique_state_dict(model)\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001B[0m, in \u001B[0;36m_trace_and_get_graph_from_model\u001B[0;34m(model, args)\u001B[0m\n\u001B[1;32m    913\u001B[0m prev_autocast_cache_enabled \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mis_autocast_cache_enabled()\n\u001B[1;32m    914\u001B[0m torch\u001B[38;5;241m.\u001B[39mset_autocast_cache_enabled(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 915\u001B[0m trace_graph, torch_out, inputs_states \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_trace_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_force_outplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_return_inputs_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    921\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    922\u001B[0m torch\u001B[38;5;241m.\u001B[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001B[1;32m    924\u001B[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/jit/_trace.py:1285\u001B[0m, in \u001B[0;36m_get_trace_graph\u001B[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001B[0m\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(args, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m   1284\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args,)\n\u001B[0;32m-> 1285\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[43mONNXTracedModule\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_force_outplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_return_inputs_states\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1288\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outs\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/jit/_trace.py:133\u001B[0m, in \u001B[0;36mONNXTracedModule.forward\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(out_vars)\n\u001B[0;32m--> 133\u001B[0m graph, out \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_graph_by_tracing\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrapper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43min_vars\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_create_interpreter_name_lookup_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_force_outplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_inputs:\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m graph, outs[\u001B[38;5;241m0\u001B[39m], ret_inputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/jit/_trace.py:124\u001B[0m, in \u001B[0;36mONNXTracedModule.forward.<locals>.wrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_inputs_states:\n\u001B[1;32m    123\u001B[0m     inputs_states\u001B[38;5;241m.\u001B[39mappend(_unflatten(in_args, in_desc))\n\u001B[0;32m--> 124\u001B[0m outs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtrace_inputs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_inputs_states:\n\u001B[1;32m    126\u001B[0m     inputs_states[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m (inputs_states[\u001B[38;5;241m0\u001B[39m], trace_inputs)\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001B[0m, in \u001B[0;36mModule._slow_forward\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m         recording_scopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1508\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1510\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m recording_scopes:\n",
      "File \u001B[0;32m~/fprojects/python/denoising/models/ConvAutoEncoder.py:98\u001B[0m, in \u001B[0;36mConvAutoencoderLuma.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 98\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_layers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001B[0m, in \u001B[0;36mModule._slow_forward\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m         recording_scopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1508\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1510\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m recording_scopes:\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001B[0m, in \u001B[0;36mModule._slow_forward\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m         recording_scopes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1508\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1510\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m recording_scopes:\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given groups=1, weight of size [76, 1, 3, 3], expected input[1, 3, 100, 100] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "path_save_model_onnx = os.path.join(\"../assets/onnx/\", f\"{name_dataset}_{name_model}.onnx\")\n",
    "image_denoising.save_onnx_model(model, path_save_model_onnx, device, image_size=(width, height))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T01:01:43.894855717Z",
     "start_time": "2023-11-06T01:01:42.987401334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%img : Float(1, 1, 100, 100, strides=[10000, 10000, 100, 1], requires_grad=0, device=cpu),\n",
      "      %all_layers.0.weight : Float(76, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %all_layers.0.bias : Float(76, strides=[1], requires_grad=1, device=cpu),\n",
      "      %all_layers.3.weight : Float(38, 76, 2, 2, strides=[304, 4, 2, 1], requires_grad=1, device=cpu),\n",
      "      %all_layers.3.bias : Float(38, strides=[1], requires_grad=1, device=cpu),\n",
      "      %all_layers.6.weight : Float(38, 38, 2, 2, strides=[152, 4, 2, 1], requires_grad=1, device=cpu),\n",
      "      %all_layers.6.bias : Float(38, strides=[1], requires_grad=1, device=cpu),\n",
      "      %all_layers.9.weight : Float(38, 38, 2, 2, strides=[152, 4, 2, 1], requires_grad=1, device=cpu),\n",
      "      %all_layers.9.bias : Float(38, strides=[1], requires_grad=1, device=cpu),\n",
      "      %all_layers.11.weight : Float(38, 76, 2, 2, strides=[304, 4, 2, 1], requires_grad=1, device=cpu),\n",
      "      %all_layers.11.bias : Float(76, strides=[1], requires_grad=1, device=cpu),\n",
      "      %all_layers.13.weight : Float(76, 1, 2, 2, strides=[4, 4, 2, 1], requires_grad=1, device=cpu),\n",
      "      %all_layers.13.bias : Float(1, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/all_layers/all_layers.0/Conv_output_0 : Float(1, 76, 100, 100, strides=[760000, 10000, 100, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/all_layers/all_layers.0/Conv\"](%img, %all_layers.0.weight, %all_layers.0.bias), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.conv.Conv2d::all_layers.0 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/all_layers/all_layers.1/Relu_output_0 : Float(1, 76, 100, 100, strides=[760000, 10000, 100, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/all_layers/all_layers.1/Relu\"](%/all_layers/all_layers.0/Conv_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.activation.ReLU::all_layers.1 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/all_layers/all_layers.2/MaxPool_output_0 : Float(1, 76, 99, 99, strides=[744876, 9801, 99, 1], requires_grad=0, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.2/MaxPool\"](%/all_layers/all_layers.1/Relu_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.pooling.MaxPool2d::all_layers.2 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:791:0\n",
      "  %/all_layers/all_layers.3/Conv_output_0 : Float(1, 38, 98, 98, strides=[364952, 9604, 98, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.3/Conv\"](%/all_layers/all_layers.2/MaxPool_output_0, %all_layers.3.weight, %all_layers.3.bias), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.conv.Conv2d::all_layers.3 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/all_layers/all_layers.4/Relu_output_0 : Float(1, 38, 98, 98, strides=[364952, 9604, 98, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/all_layers/all_layers.4/Relu\"](%/all_layers/all_layers.3/Conv_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.activation.ReLU::all_layers.4 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/all_layers/all_layers.5/MaxPool_output_0 : Float(1, 38, 97, 97, strides=[357542, 9409, 97, 1], requires_grad=0, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.5/MaxPool\"](%/all_layers/all_layers.4/Relu_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.pooling.MaxPool2d::all_layers.5 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:791:0\n",
      "  %/all_layers/all_layers.6/Conv_output_0 : Float(1, 38, 96, 96, strides=[350208, 9216, 96, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.6/Conv\"](%/all_layers/all_layers.5/MaxPool_output_0, %all_layers.6.weight, %all_layers.6.bias), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.conv.Conv2d::all_layers.6 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/all_layers/all_layers.7/Relu_output_0 : Float(1, 38, 96, 96, strides=[350208, 9216, 96, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/all_layers/all_layers.7/Relu\"](%/all_layers/all_layers.6/Conv_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.activation.ReLU::all_layers.7 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/all_layers/all_layers.8/MaxPool_output_0 : Float(1, 38, 97, 97, strides=[357542, 9409, 97, 1], requires_grad=0, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/all_layers/all_layers.8/MaxPool\"](%/all_layers/all_layers.7/Relu_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.pooling.MaxPool2d::all_layers.8 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:791:0\n",
      "  %/all_layers/all_layers.9/ConvTranspose_output_0 : Float(1, 38, 98, 98, strides=[364952, 9604, 98, 1], requires_grad=0, device=cpu) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.9/ConvTranspose\"](%/all_layers/all_layers.8/MaxPool_output_0, %all_layers.9.weight, %all_layers.9.bias), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.conv.ConvTranspose2d::all_layers.9 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952:0\n",
      "  %/all_layers/all_layers.10/Relu_output_0 : Float(1, 38, 98, 98, strides=[364952, 9604, 98, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/all_layers/all_layers.10/Relu\"](%/all_layers/all_layers.9/ConvTranspose_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.activation.ReLU::all_layers.10 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/all_layers/all_layers.11/ConvTranspose_output_0 : Float(1, 76, 99, 99, strides=[744876, 9801, 99, 1], requires_grad=0, device=cpu) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.11/ConvTranspose\"](%/all_layers/all_layers.10/Relu_output_0, %all_layers.11.weight, %all_layers.11.bias), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.conv.ConvTranspose2d::all_layers.11 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952:0\n",
      "  %/all_layers/all_layers.12/Relu_output_0 : Float(1, 76, 99, 99, strides=[744876, 9801, 99, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/all_layers/all_layers.12/Relu\"](%/all_layers/all_layers.11/ConvTranspose_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.activation.ReLU::all_layers.12 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/all_layers/all_layers.13/ConvTranspose_output_0 : Float(1, 1, 100, 100, strides=[10000, 10000, 100, 1], requires_grad=0, device=cpu) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/all_layers/all_layers.13/ConvTranspose\"](%/all_layers/all_layers.12/Relu_output_0, %all_layers.13.weight, %all_layers.13.bias), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.conv.ConvTranspose2d::all_layers.13 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952:0\n",
      "  %output : Float(1, 1, 100, 100, strides=[10000, 10000, 100, 1], requires_grad=0, device=cpu) = onnx::Sigmoid[onnx_name=\"/all_layers/all_layers.14/Sigmoid\"](%/all_layers/all_layers.13/ConvTranspose_output_0), scope: models.ConvAutoEncoder.ConvAutoencoderLuma::/torch.nn.modules.container.Sequential::all_layers/torch.nn.modules.activation.Sigmoid::all_layers.14 # /home/amedvedev/fprojects/python/denoising/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:292:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        inp = torch.randn((1, 1, width, height), device=device)\n",
    "\n",
    "        torch.onnx.export(model,\n",
    "                          (inp,),\n",
    "                          path_save_model_onnx,\n",
    "                          verbose=True,\n",
    "                          input_names=(\"img\",),\n",
    "                          output_names=(\"output\",),\n",
    "                          opset_version=14,\n",
    "                          do_constant_folding=False,\n",
    "                          export_params=True,\n",
    "                          dynamic_axes=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T01:02:53.479352700Z",
     "start_time": "2023-11-06T01:02:53.378879708Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
